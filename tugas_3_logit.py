# -*- coding: utf-8 -*-
"""tugas-3-logit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Jcw6_NkfIXrfXBNGAT7g0eZjTM3AKGav

# Implementasi VSM TF-IDF menggunakan Klasifikasi Logistic Regression

Fungsi dari logistic regression adalah sebagai berikut, menggunakan sigmoid function

$$
\sigma(x) = \frac{1}{1+e^{-x}}
$$

fungsi logistic/sigmoid hanya mereturnkan angka dari range 0-1
"""

# untuk plotting sigmoid
import numpy as np
import math

# Library untuk data manipulation & visualisasi
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Library untuk preprocessing data
from sklearn import preprocessing
from sklearn.model_selection import train_test_split

# Library untuk model & evaluasi
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

# Library untuk save model
import pickle

x = np.linspace(-10, 10, 100)
z = 1/(1 + np.exp(-x))

plt.plot(x, z)
plt.xlabel("x")
plt.ylabel("Sigmoid(X)")

plt.show()

"""1. **pandas** digunakan untuk memanipulasi data
2. **matplotlib & seaborn** untuk visualisasi grafik
3. **preprocessing** untuk melakukan proses label encoding
4. **train_test_split** untuk membagi data train dan test
5. **LogisticRegression** digunakan untuk modelling logistic regression
6. **classification_report & confussion_matrix** digunakan untuk melihat hasil evaluasi atau laporan setelah proses modelling
7. **pickle** untuk menyimpan model

## Load dataset
"""

data_train = pd.read_csv('data_train_vsm.csv')
data_test = pd.read_csv('data_test_vsm.csv')

data_train.head(10)

data_test.head(10)

"""## Encoding label dataset

Kategori berita adalah variabel Y yang ingin diprediksi, dikarenakan masih dalam bentuk kategorik maka harus diubah ke dalam numerik dengan label encoding
"""

label_encoder = preprocessing.LabelEncoder()

# Encode Train Kategori Berita
data_train['Kategori Berita'] = label_encoder.fit_transform(data_train['Kategori Berita'])
data_test['Kategori Berita'] = label_encoder.fit_transform(data_test['Kategori Berita'])

data_train.head(10)

"""## Split dataset (train & test)
Memisahkan dataset antara train dan test dengan train 70% dari datase dan test 30% dari dataset
"""

X_train = data_train.drop(['Kategori Berita'], axis=1)
y_train = data_train['Kategori Berita']

X_test = data_test.drop(['Kategori Berita'], axis=1)
y_test = data_test['Kategori Berita']

X_test

"""## Modelling

membuat model yang sebelumnya sudah displit
"""

model = LogisticRegression()
model.fit(X_train, y_train)

"""### Testing

memprediksi datset dengan model yang sudah dibuat
"""

# Make predictions
y_pred = model.predict(X_test)

a = pd.DataFrame({'Data ori':y_test, 'Data tebak':y_pred})
a

"""## Evaluasi Model"""

#Confusion matrix and classification report
matrix = confusion_matrix(y_test, y_pred)
sns.heatmap(matrix, annot=True, fmt="d")
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
print(classification_report(y_test, y_pred))

"""Dari model diatas terlihat bahwasanya akurasi sebesar 90%

## Saving model ke pickle
"""

with open('lr_model.pkl', 'wb') as f:
    pickle.dump(model, f)

# prompt: buat api flask

from flask import Flask, request, jsonify
import pickle
import pandas as pd
import numpy as np

app = Flask(__name__)

# Load the trained model
with open('lr_model.pkl', 'rb') as f:
    model = pickle.load(f)

@app.route('/predict', methods=['POST'])
def predict():
    try:
        # Get data from the request
        data = request.get_json()

        # Convert the JSON data to a NumPy array
        input_data = np.array(list(data.values())).reshape(1,-1)  # Reshape for single prediction

        # Make predictions using the loaded model
        prediction = model.predict(input_data)

        # Return the prediction as a JSON response
        return jsonify({'prediction': int(prediction[0])})

    except Exception as e:
        return jsonify({'error': str(e)})

# prompt: deploy model ke streamlit

# This code is designed to run on a local machine, not within a Google Colab environment.
# To deploy this to Streamlit, you will need to install the necessary packages locally.
#
# 1. Save the code below as app.py
# 2. Navigate to the directory containing app.py in your terminal.
# 3. Install Streamlit and other necessary libraries:
#    pip install streamlit pandas scikit-learn numpy matplotlib seaborn
# 4. Run the app using: streamlit run app.py


import streamlit as st
import pickle
import numpy as np
import pandas as pd

# Load the trained model
with open('lr_model.pkl', 'rb') as f:
    model = pickle.load(f)

# Create the Streamlit app
st.title("Berita Classifier")

# Input fields for the features (replace with your actual feature names)
feature1 = st.number_input("Feature 1", value=0.0)
feature2 = st.number_input("Feature 2", value=0.0)
# ... add input fields for all features ...


# Create a button to trigger prediction
if st.button("Predict"):
    # Create input array
    input_data = np.array([feature1, feature2]).reshape(1,-1) # Assuming you have 2 features

    # Make predictions
    prediction = model.predict(input_data)

    # Display the prediction
    st.write(f"Prediction: {int(prediction[0])}")